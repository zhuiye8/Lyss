package kb

import (
	"bytes"
	"errors"
	"io"
	"mime/multipart"
	"path/filepath"
	"strings"
	"time"

	"github.com/google/uuid"
)

// DocumentType è¡¨ç¤ºæ–‡æ¡£ç±»å‹
type DocumentType string

const (
	// TypeText çº¯æ–‡æœ¬æ–‡æ¡?
	TypeText DocumentType = "text"
	// TypeMarkdown Markdownæ–‡æ¡£
	TypeMarkdown DocumentType = "markdown"
	// TypePDF PDFæ–‡æ¡£
	TypePDF DocumentType = "pdf"
	// TypeWord Wordæ–‡æ¡£
	TypeWord DocumentType = "docx"
	// TypeExcel Excelæ–‡æ¡£
	TypeExcel DocumentType = "xlsx"
	// TypeHTML HTMLæ–‡æ¡£
	TypeHTML DocumentType = "html"
)

// Document è¡¨ç¤ºä¸€ä¸ªæ–‡æ¡?
type Document struct {
	ID             string       `json:"id"`
	KnowledgeBaseID string       `json:"knowledge_base_id"`
	Name           string       `json:"name"`
	Type           DocumentType `json:"type"`
	Size           int64        `json:"size"`
	Content        string       `json:"-"` // åŸå§‹å†…å®¹ï¼Œä¸åœ¨JSONä¸­è¿”å›?
	CreatedAt      time.Time    `json:"created_at"`
	UpdatedAt      time.Time    `json:"updated_at"`
	Metadata       interface{}  `json:"metadata,omitempty"`
}

// Chunk è¡¨ç¤ºæ–‡æ¡£çš„ä¸€ä¸ªåˆ†å?
type Chunk struct {
	ID         string    `json:"id"`
	DocumentID string    `json:"document_id"`
	Content    string    `json:"content"`
	Vector     []float32 `json:"-"`
	Metadata   struct {
		ChunkIndex int    `json:"chunk_index"`
		PageNumber int    `json:"page_number,omitempty"`
		Source     string `json:"source"`
	} `json:"metadata"`
}

// DocumentProcessor æ–‡æ¡£å¤„ç†å™¨æ¥å?
type DocumentProcessor interface {
	// Process å¤„ç†æ–‡æ¡£å¹¶è¿”å›åˆ†å—ç»“æ?
	Process(doc *Document) ([]Chunk, error)
	// SupportsType æ£€æŸ¥æ˜¯å¦æ”¯æŒç‰¹å®šæ–‡æ¡£ç±»å?
	SupportsType(docType DocumentType) bool
}

// DocumentProcessorRegistry æ–‡æ¡£å¤„ç†å™¨æ³¨å†Œè¡¨
type DocumentProcessorRegistry struct {
	processors map[DocumentType]DocumentProcessor
}

// NewDocumentProcessorRegistry åˆ›å»ºæ–°çš„æ–‡æ¡£å¤„ç†å™¨æ³¨å†Œè¡¨
func NewDocumentProcessorRegistry() *DocumentProcessorRegistry {
	return &DocumentProcessorRegistry{
		processors: make(map[DocumentType]DocumentProcessor),
	}
}

// Register æ³¨å†Œæ–‡æ¡£å¤„ç†å™?
func (r *DocumentProcessorRegistry) Register(docType DocumentType, processor DocumentProcessor) {
	r.processors[docType] = processor
}

// GetProcessor è·å–æŒ‡å®šç±»å‹çš„å¤„ç†å™¨
func (r *DocumentProcessorRegistry) GetProcessor(docType DocumentType) (DocumentProcessor, error) {
	processor, exists := r.processors[docType]
	if !exists {
		return nil, errors.New("no processor found for document type: " + string(docType))
	}
	return processor, nil
}

// ProcessDocument å¤„ç†æ–‡æ¡£
func (r *DocumentProcessorRegistry) ProcessDocument(doc *Document) ([]Chunk, error) {
	processor, err := r.GetProcessor(doc.Type)
	if err != nil {
		return nil, err
	}
	return processor.Process(doc)
}

// NewDocument ä»æ–‡ä»¶åˆ›å»ºæ–°æ–‡æ¡£
func NewDocument(knowledgeBaseID string, file *multipart.FileHeader) (*Document, error) {
	// æ‰“å¼€æ–‡ä»¶
	f, err := file.Open()
	if err != nil {
		return nil, err
	}
	defer f.Close()
	
	// è¯»å–æ–‡ä»¶å†…å®¹
	buf := bytes.NewBuffer(nil)
	if _, err := io.Copy(buf, f); err != nil {
		return nil, err
	}
	
	// è·å–æ–‡ä»¶ç±»å‹
	ext := strings.ToLower(filepath.Ext(file.Filename))
	docType := getDocumentTypeFromExt(ext)
	
	now := time.Now()
	doc := &Document{
		ID:             uuid.New().String(),
		KnowledgeBaseID: knowledgeBaseID,
		Name:           file.Filename,
		Type:           docType,
		Size:           file.Size,
		Content:        buf.String(),
		CreatedAt:      now,
		UpdatedAt:      now,
	}
	
	return doc, nil
}

// NewTextDocument ä»æ–‡æœ¬åˆ›å»ºæ–°æ–‡æ¡£
func NewTextDocument(knowledgeBaseID, name, content string, docType DocumentType) *Document {
	now := time.Now()
	return &Document{
		ID:             uuid.New().String(),
		KnowledgeBaseID: knowledgeBaseID,
		Name:           name,
		Type:           docType,
		Size:           int64(len(content)),
		Content:        content,
		CreatedAt:      now,
		UpdatedAt:      now,
	}
}

// ä»æ–‡ä»¶æ‰©å±•åè·å–æ–‡æ¡£ç±»å‹
func getDocumentTypeFromExt(ext string) DocumentType {
	switch ext {
	case ".txt":
		return TypeText
	case ".md", ".markdown":
		return TypeMarkdown
	case ".pdf":
		return TypePDF
	case ".doc", ".docx":
		return TypeWord
	case ".xls", ".xlsx":
		return TypeExcel
	case ".html", ".htm":
		return TypeHTML
	default:
		return TypeText
	}
}

// DefaultChunkSize é»˜è®¤çš„æ–‡æœ¬åˆ†å—å¤§å°?
const DefaultChunkSize = 1000

// DefaultChunkOverlap é»˜è®¤çš„æ–‡æœ¬åˆ†å—é‡å å¤§å°?
const DefaultChunkOverlap = 200

// BasicTextProcessor åŸºç¡€æ–‡æœ¬å¤„ç†å™?
type BasicTextProcessor struct {
	ChunkSize    int
	ChunkOverlap int
}

// NewBasicTextProcessor åˆ›å»ºåŸºç¡€æ–‡æœ¬å¤„ç†å™?
func NewBasicTextProcessor(chunkSize, chunkOverlap int) *BasicTextProcessor {
	if chunkSize <= 0 {
		chunkSize = DefaultChunkSize
	}
	if chunkOverlap <= 0 {
		chunkOverlap = DefaultChunkOverlap
	}
	return &BasicTextProcessor{
		ChunkSize:    chunkSize,
		ChunkOverlap: chunkOverlap,
	}
}

// Process å¤„ç†æ–‡æ¡£å¹¶è¿”å›åˆ†å?
func (p *BasicTextProcessor) Process(doc *Document) ([]Chunk, error) {
	content := doc.Content
	
	// å¦‚æœæ–‡æ¡£ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
	if len(content) == 0 {
		return []Chunk{}, nil
	}
	
	// ç®€å•æŒ‰ç…§å¥å­æˆ–æ®µè½åˆ†å‰²
	paragraphs := strings.Split(content, "\n\n")
	var chunks []Chunk
	
	currentChunk := ""
	chunkIndex := 0
	
	for _, para := range paragraphs {
		if len(currentChunk)+len(para) > p.ChunkSize {
			// å½“å‰å—å·²ç»è¶³å¤Ÿå¤§ï¼Œåˆ›å»ºä¸€ä¸ªæ–°å?
			if len(currentChunk) > 0 {
				chunks = append(chunks, Chunk{
					ID:         uuid.New().String(),
					DocumentID: doc.ID,
					Content:    currentChunk,
					Metadata: struct {
						ChunkIndex int    `json:"chunk_index"`
						PageNumber int    `json:"page_number,omitempty"`
						Source     string `json:"source"`
					}{
						ChunkIndex: chunkIndex,
						Source:     doc.Name,
					},
				})
				chunkIndex++
				
				// ä½¿ç”¨é‡å éƒ¨åˆ†åˆ›å»ºä¸‹ä¸€ä¸ªå—
				lastWords := getLastWords(currentChunk, p.ChunkOverlap)
				currentChunk = lastWords
			}
		}
		
		if len(currentChunk) > 0 {
			currentChunk += "\n\n"
		}
		currentChunk += para
	}
	
	// æ·»åŠ æœ€åä¸€ä¸ªå—
	if len(currentChunk) > 0 {
		chunks = append(chunks, Chunk{
			ID:         uuid.New().String(),
			DocumentID: doc.ID,
			Content:    currentChunk,
			Metadata: struct {
				ChunkIndex int    `json:"chunk_index"`
				PageNumber int    `json:"page_number,omitempty"`
				Source     string `json:"source"`
			}{
				ChunkIndex: chunkIndex,
				Source:     doc.Name,
			},
		})
	}
	
	return chunks, nil
}

// SupportsType æ£€æŸ¥æ˜¯å¦æ”¯æŒç‰¹å®šæ–‡æ¡£ç±»å?
func (p *BasicTextProcessor) SupportsType(docType DocumentType) bool {
	return docType == TypeText || docType == TypeMarkdown
}

// è·å–æ–‡æœ¬çš„æœ€ånä¸ªå­—ç¬¦ï¼Œç¡®ä¿ä¸ä¼šæ‹†åˆ†å•è¯
func getLastWords(text string, n int) string {
	if len(text) <= n {
		return text
	}
	
	// æŸ¥æ‰¾é€‚å½“çš„æ–­ç‚?
	cutIndex := len(text) - n
	for i := cutIndex; i < len(text); i++ {
		if text[i] == ' ' || text[i] == '\n' {
			cutIndex = i + 1
			break
		}
	}
	
	return text[cutIndex:]
}

// DefaultProcessorRegistry é»˜è®¤çš„æ–‡æ¡£å¤„ç†å™¨æ³¨å†Œè¡?
var DefaultProcessorRegistry = NewDocumentProcessorRegistry()

// åˆå§‹åŒ–é»˜è®¤å¤„ç†å™¨
func init() {
	basicProcessor := NewBasicTextProcessor(DefaultChunkSize, DefaultChunkOverlap)
	DefaultProcessorRegistry.Register(TypeText, basicProcessor)
	DefaultProcessorRegistry.Register(TypeMarkdown, basicProcessor)
} 
